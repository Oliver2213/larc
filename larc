#!/usr/bin/python

# The literotica archiver
# Author: Blake Oliver <oliver22213@me.com>

from collections import deque
import datetime
import os
import shutil
import subprocess
import sys
import threading

import click
import requests


@click.command()
@click.option('--archive-dir', '--dir', 'archive_dir', default='./literotica', help="Directory to store archive files", type=click.Path(exists=True, writable=True))
@click.option('--base-url', default='https://literotica.com/', help='Base URL for literotica')
@click.option('--thread-count', default=2, show_default=True, help="The number of download worker threads")
@click.option('-z', '--zip-archive', default=False, help='Zip the archive directory after downloading stories')
@click.option('-zn', '--zip-name', default='literotica_archive_'+str(datetime.datetime.now()), help='Filename for the zip archive without the .zip extention')

@click.argument('category-file', type=click.Path(file_okay=True, exists=True, readable=True))
def literoticarchiver(archive_dir, base_url, category_file, thread_count, zip_archive, zip_name):
    """ Literoticarchiver - A script to archive the given literotica categories.

Category_file is a path to a file that contains the categories you want archived, one per line.

A directory for each category will be created.

All stories are stored in epub form; re-running this script will update any previously stored stories and download new ones."""
    
    click.echo("Starting literotica archival...")
    with open(category_file, 'r') as f:
        # l = f.readlines()
        l = f.read().split('\n')
        if l[-1]=='': del(l[-1])
        click.echo("Category file opened.")
        url_list=deque()
        click.echo("Starting {} download worker threads.".format(thread_count))
        finished_retrieving_urls = threading.Event()
        threads = []
        for r in range(thread_count):
            t = threading.Thread(target=download_worker, args=(url_list, archive_dir, finished_retrieving_urls,))
            t.start()
            threads.append(t)
        for category in l:
            category = unicode(category)
            category.strip()
            click.echo("Processing category %s" % category)
            if os.path.exists(os.path.join(archive_dir, category)) == False: # a directory for this category doesn't exist
                click.echo("Making category directory.")
                os.makedirs(os.path.join(archive_dir, category))
            page=1
            click.echo("Starting story url retrieval for category %s" % category)
            running=True
            while running:
                url = '{0}{1}{2}{3}'.format(base_url, 'c/', category+'/', str(page)+'-page/')
                click.echo("Using URL %s" % (url))
                r=requests.head(url) 
                click.echo("Status code is %s." %(r.status_code))
                if r.status_code==200: # The category page url is valid; extract valid story urls
                    u = subprocess.check_output(['fanficfare', '-n', url]) # get a list of valid storyurls from the category page
                    u = u.split('\n') # split output by lines
                    if u[-1] == '': del(u[-1])
                    u_t = []
                    # associate every url with the current category in a tuple, so later we can send the downloaded story to the right place
                    for url in u:
                        t = url, category
                        u_t.append(t)
                    url_list.extend(u_t) # add our urls to the main list of ones to be downloaded
                    page+=1
                elif r.status_code==301: # no more pages; literotica is trying to redirect us back to the last valid page
                    page=page-1 # the last increment got us the 301
                    click.echo("Finished retrieving urls for category {0}; {1} total pages.".format(category, str(page)))
                    running=False # end the category page retrieval loop
        finished_retrieving_urls.set() # done collecting urls now; if threads notice that the deque is empty they can exit

    if zip_archive:
        shutil.make_archive(zip_name, "zip", archive_dir)


def download_worker(d, archive_dir, finished_retrieving_urls):
    """Worker function that pops and downloads a url off the provided deque."""
    while True:
        try:
            u = d.pop()
            subprocess.call(['fanficfare', '--update-epub', u[0]], cwd=os.path.join(archive_dir, u[1]))
        except IndexError: # No more items in the deque to pop
            if finished_retrieving_urls.isSet(): # if we're done getting urls
                break # if the deque is empty and we have all the urls our job is done
    return


if __name__ == '__main__':
    literoticarchiver()