#!/usr/bin/python

# The literotica archiver
# Author: Blake Oliver <oliver22213@me.com>

from collections import deque
import datetime
import logging
import os
import re
import shutil
import subprocess
import sys
import threading

import click
from bs4 import BeautifulSoup
import requests

# Constants:
le_storyurl_regexp = re.compile(r"""^https://www\.literotica\.com/s/.+""")
le_categoryurl_regexp = re.compile(r"""https://www\.literotica\.com/c/.+""")

def output_categories(ctx, param, val):
    """Callback function for the '--output-categories' option; extracts all story categories from literotica and outputs  all category names, commented, to the given text file
    """
    if not val or ctx.resilient_parsing:
        return
    click.echo("Writing all Literotica categories, commented, to {}.".format(click.format_filename(val)))
    stories_page = requests.get("https://www.literotica.com/stories")
    categories = extract_categoryurls(stories_page.content)
    click.echo("{} categories found; writing...".format(len(categories)))
    with click.open_file(val, 'w') as f:
        f.write("# Larc categories file\n")
        f.write("# Uncomment the categories you want larc to archive by removing the number in front of each; leaving the space doesn't matter\n")
        f.write("\n")
        for c in categories:
            f.write("# "+c+"\n")
    click.echo("Finished.")
    click.echo("Please uncomment the categories you want archived by removing the number in front of each; leaving or removing the space as well doesn't matter.")
    ctx.exit()


@click.command()
@click.option('--archive-dir', '--dir', 'archive_dir', default='./literotica', help="Directory to store archive files", type=click.Path(exists=True, writable=True))
@click.option('--base-url', default='https://literotica.com/', help='Base URL for literotica')
@click.option('--dry-run', '-d', default=False, is_flag=True, help='Collect URLs from categories specified in category_file, but do not start download worker threads')
@click.option('--output-categories', '-oc', callback=output_categories, expose_value=False, is_eager=True, type=click.Path(file_okay=True, writable=True), help="Output all Literotica categories to the given file. Each category name will be commented with a #; to choose which ones you want archived when providing this file as category_file, just remove the number (#) from in front of each name. Leaving the space there or taking it out doesn't matter.")
@click.option('--quiet', '-q', 'verbosity', flag_value='warn', help='Be quiet unless there\'s a warning or error')
@click.option('--thread-count', default=2, show_default=True, help="The number of download worker threads")
@click.option('--verbose', '-v', 'verbosity', flag_value='debug', help='Be very descriptive about what\'s happening')
@click.option('-z', '--zip-archive', default=False, help='Zip the archive directory after downloading stories')
@click.option('-zn', '--zip-name', default='literotica_archive_'+str(datetime.datetime.now()), help='Filename for the zip archive without the .zip extention')

@click.argument('category-file', type=click.Path(file_okay=True, exists=True, readable=True))
def literoticarchiver(archive_dir, base_url, category_file, thread_count, verbosity, zip_archive, zip_name, dry_run):
    """ Larc - A script to archive the given literotica categories.

Category_file is a path to a file that contains the categories you want archived, one per line.

A directory for each category will be created.

All stories are stored in epub form; re-running this script will update any previously stored stories and download new ones."""
    
    # Setup logging
    log = logging.getLogger("larc")
    log.setLevel(logging.DEBUG) # set the main logger to the lowest level so all messages get passed to all handlers that will take them
    ch = logging.StreamHandler() # console handler
    ch_f = logging.Formatter('[%(name)s] (%(threadName)s) - %(levelname)s: %(message)s') # console formatter
    ch.setFormatter(ch_f)
    log.addHandler(ch)
    # Set the accepted log level of the console handler only
    if verbosity == 'debug':
        ch.setLevel(logging.DEBUG)
    elif verbosity == 'warn':
        ch.setLevel(logging.WARN)
    else: # no specific level requested; default to info
        ch.setLevel(logging.INFO)

    if not dry_run:
        log.info("Starting literotica archival...")
    else:
        log.info("Dry_run mode active; collecting URLs for categories, but not starting download worker threads")
    try: # huge try / except block to handle KeyboardInterrupt
        with open(category_file, 'r') as f:
            log.debug("Category file {} opened".format(click.format_filename(category_file)))
            l = f.read().split('\n')
            log.debug("Category file split by lines.")
        log.debug("Removing comments and blank lines from the list of categories")
        l_len = len(l)
        l[:] = [line for line in l if is_comment(line)==False]
        l[:] = [line.strip() for line in l]
        log.debug("{} comments and blank lines removed from categories list".format(l_len-len(l)))
        log.debug("Category file processed.")
        url_list=deque()
        urls_processed = 0
        urls_total = 0
        if not dry_run:
            log.info("Starting {} download worker threads.".format(thread_count))
            finished_retrieving_urls = threading.Event()
            stop_now = threading.Event()
            threads = []
            log.debug("Starting download worker threads")
            for r in range(thread_count):
                t = threading.Thread(target=download_worker, args=(url_list, archive_dir, finished_retrieving_urls, log, stop_now,))
                t.start()
                threads.append(t)
            log.debug("Finished starting worker threads")
        log.debug("Starting category processing; {} total categories".format(len(l)))
        for category in l:
            category.strip()
            log.info("Working on category {}".format(category))
            if os.path.exists(os.path.join(archive_dir, category)) == False: # a directory for this category doesn't exist
                log.debug("Making category directory.")
                os.makedirs(os.path.join(archive_dir, category))
            page=1
            log.debug("Starting off on page {}".format(page))
            log.info("Starting story url retrieval")
            running=True
            while running:
                url = '{0}{1}{2}{3}'.format(base_url, 'c/', category+'/', str(page)+'-page/')
                log.debug("Using URL {}".format(url))
                r=requests.get(url, allow_redirects=False)
                log.debug("Status code for {} is {}".format(url, r.status_code))
                if r.status_code==200: # The category page url is valid; extract valid story urls
                    log.debug("Status code valid, passing url to extract_storyurls")
                    u = extract_storyurls(r.content)
                    log.debug("Finished storyurl extraction")
                    log.debug("{} total urls extracted from page".format(len(u)))
                    u_t = []
                    # associate every url with the current category in a tuple, so later we can send the downloaded story to the right place
                    log.debug("Starting url-to-category asociation loop")
                    for url in u:
                        t = url, category
                        u_t.append(t)
                    log.debug("Finished creating an amalgamated url list")
                    url_list.extend(u_t) # add our urls to the main list of ones to be downloaded
                    log.debug("{} URLs added to the deque".format(len(u_t)))
                    urls_total += len(u_t)
                    page+=1
                    log.debug("Working on page {}".format(page))
                elif r.status_code==301: # no more pages; literotica is trying to redirect us back to the last valid page
                    page=page-1 # the last increment got us the 301
                    log.info("Finished retrieving urls for category {0}; {1} total pages.".format(category, str(page)))
                    running=False # end the category page retrieval loop
        log.debug("Finished retrieving URLs; {} retrieved in all.".format(len(urls_total)))
        if not dry_run:
            log.debug("Setting flag so worker threads know it's safe to exit if the deque is empty")
            finished_retrieving_urls.set() # done collecting urls now; if threads notice that the deque is empty they can exit

        if zip_archive and not dry_run:
            log.info("Zipping up the archive directory...")
            shutil.make_archive(zip_name, "zip", archive_dir)
            log.info("Finished zipping")
    except KeyboardInterrupt:
        if not dry_run:
            # We've got some threads to kill
            # The idea here is to set one flag, and all worker threads will notice it in their while loops and stop, then return
            stop_now.set()

def download_worker(d, archive_dir, finished_retrieving_urls, log, stop_now):
    """Worker function that pops and downloads a url off the provided deque."""
    log.debug("Worker started")
    while not stop_now.isSet(): # As long as the stop_now flag isn't set, we can loop
        try:
            u = d.pop()
            subprocess.call(['fanficfare', '--update-epub', u[0]], cwd=os.path.join(archive_dir, u[1]), stdout=None)
            urls_processed += 1
        except IndexError: # No more items in the deque to pop
            if finished_retrieving_urls.isSet(): # if we're done getting urls
                log.debug("Deque is empty and we're done retrieving URLs")
                break # if the deque is empty and we have all the urls our job is done
    log.debug("Exiting")
    return

def extract_storyurls(p):
    """Given a page from literotica, extract all of the storyurls present
    The passed object must be a string
    returns: a list of urls
    """
    s = BeautifulSoup(p)
    l = s.find_all(href=le_storyurl_regexp) # find all tags that have href as a keyword that match our literotica storyurl regexp
    l_t = []
    for i in l: # iterate through the list of matching urls
        l_t.append(i['href'])
    return l_t

def extract_categoryurls(p):
    """Given a page from Literotica, extract all categories from it
    The past object must be a string.
    returns: a list of category names
    """
    s = BeautifulSoup(p)
    l = s.find_all(href=le_categoryurl_regexp)
    l_t = []
    for i in l:
        l_t.append(i['href'].split('/')[-1]) # split up the category url by /, and add the last component to l_t
    return l_t

def is_comment(line):
    """Return true if the provided line is a comment (that is, if it starts with a number, is an empty string, or just a newline)
    """
    if line.lstrip().startswith('#') or line == '' or line == '\n':
        return True # provided string is a comment
    else:
        return False

if __name__ == '__main__':
    literoticarchiver()