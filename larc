#!/usr/bin/python

# The literotica archiver
# Author: Blake Oliver <oliver22213@me.com>

from collections import deque
import datetime
import logging
import os
import re
import shutil
import subprocess
import sys
import threading

import click
from bs4 import BeautifulSoup
import requests

# Constants:
le_storyurl_regexp = re.compile(r"""^https://www\.literotica\.com/s/.+""")

@click.command()
@click.option('--archive-dir', '--dir', 'archive_dir', default='./literotica', help="Directory to store archive files", type=click.Path(exists=True, writable=True))
@click.option('--base-url', default='https://literotica.com/', help='Base URL for literotica')
@click.option('--quiet', '-q', 'verbosity', flag_value='warn', help='Be quiet unless there\'s a warning or error')
@click.option('--thread-count', default=2, show_default=True, help="The number of download worker threads")
@click.option('--verbose', '-v', 'verbosity', flag_value='debug', help='Be very descriptive about what\'s happening')
@click.option('-z', '--zip-archive', default=False, help='Zip the archive directory after downloading stories')
@click.option('-zn', '--zip-name', default='literotica_archive_'+str(datetime.datetime.now()), help='Filename for the zip archive without the .zip extention')

@click.argument('category-file', type=click.Path(file_okay=True, exists=True, readable=True))
def literoticarchiver(archive_dir, base_url, category_file, thread_count, verbosity, zip_archive, zip_name):
    """ Larc - A script to archive the given literotica categories.

Category_file is a path to a file that contains the categories you want archived, one per line.

A directory for each category will be created.

All stories are stored in epub form; re-running this script will update any previously stored stories and download new ones."""
    
    # Setup logging
    log = logging.getLogger("larc")
    log.setLevel(logging.DEBUG) # set the main logger to the lowest level so all messages get passed to all handlers that will take them
    ch = logging.StreamHandler() # console handler
    ch_f = logging.Formatter('[%(name)s] (%(threadName)s) - %(levelname)s: %(message)s') # console formatter
    ch.setFormatter(ch_f)
    log.addHandler(ch)
    # Set the accepted log level of the console handler only
    if verbosity == 'debug':
        ch.setLevel(logging.DEBUG)
    elif verbosity == 'warn':
        ch.setLevel(logging.WARN)
    else: # no specific level requested; default to info
        ch.setLevel(logging.INFO)

    click.echo("Starting literotica archival...")
    try: # huge try / except block to handle KeyboardInterrupt
        with open(category_file, 'r') as f:
            log.debug("Category file {} opened".format(category_file))
            l = f.read().split('\n')
            log.debug("Category file split by lines.")
        if l[-1]=='': del(l[-1])
        log.info("Category file processed.")
        url_list=deque()
        log.info("Starting {} download worker threads.".format(thread_count))
        finished_retrieving_urls = threading.Event()
        stop_now = threading.Event()
        threads = []
        log.debug("Starting download worker threads")
        for r in range(thread_count):
            t = threading.Thread(target=download_worker, args=(url_list, archive_dir, finished_retrieving_urls, log, stop_now,))
            t.start()
            threads.append(t)
        log.debug("Finished starting worker threads")
        log.debug("Starting category processing; {} total categories".format(len(l)))
        for category in l:
            category = unicode(category)
            category.strip()
            log.info("Working on category {}".format(category))
            if os.path.exists(os.path.join(archive_dir, category)) == False: # a directory for this category doesn't exist
                log.debug("Making category directory.")
                os.makedirs(os.path.join(archive_dir, category))
            page=1
            log.debug("Starting off on page {}".format(page))
            log.info("Starting story url retrieval")
            running=True
            while running:
                url = '{0}{1}{2}{3}'.format(base_url, 'c/', category+'/', str(page)+'-page/')
                log.debug("Using URL {}".format(url))
                r=requests.get(url) 
                log.debug("Status code for {} is {}".format(url, r.status_code))
                if r.status_code==200: # The category page url is valid; extract valid story urls
                    log.debug("Status code valid, passing url to extract_storyurls")
                    u = extract_storyurls(r.content)
                    log.debug("Finished storyurl extraction")
                    log.debug("{} total urls extracted from page".format(len(u)))
                    u_t = []
                    # associate every url with the current category in a tuple, so later we can send the downloaded story to the right place
                    log.debug("Starting url-to-category asociation loop")
                    for url in u:
                        t = url, category
                        u_t.append(t)
                    log.debug("Finished creating an amalgamated url list")
                    url_list.extend(u_t) # add our urls to the main list of ones to be downloaded
                    log.debug("{} URLs added to the deque".format(len(u_t)))
                    page+=1
                    log.debug("Working on page {}".format(page))
                elif r.status_code==301: # no more pages; literotica is trying to redirect us back to the last valid page
                    page=page-1 # the last increment got us the 301
                    log.info("Finished retrieving urls for category {0}; {1} total pages.".format(category, str(page)))
                    running=False # end the category page retrieval loop
        log.debug("Finished retrieving all URLs; setting flag so worker threads know it's safe to exit if the deque is empty")
        finished_retrieving_urls.set() # done collecting urls now; if threads notice that the deque is empty they can exit

        if zip_archive:
            log.info("Zipping up the archive directory...")
            shutil.make_archive(zip_name, "zip", archive_dir)
            log.info("Finished zipping")
    except KeyboardInterrupt:
        # We've got some threads to kill
        # The idea here is to set one flag, and all worker threads will notice it in their while loops and stop, then return
        stop_now.set()
        # current_thread = threading.currentThread()
        # for t in threading.enumerate():
            # if t is current_thread:
                # continue
            # t.join()

def download_worker(d, archive_dir, finished_retrieving_urls, log, stop_now):
    """Worker function that pops and downloads a url off the provided deque."""
    log.debug("Worker started")
    while not stop_now.isSet(): # As long as the stop_now flag isn't set, we can loop
        try:
            u = d.pop()
            subprocess.call(['fanficfare', '--update-epub', u[0]], cwd=os.path.join(archive_dir, u[1]), stdout=None)
        except IndexError: # No more items in the deque to pop
            if finished_retrieving_urls.isSet(): # if we're done getting urls
                log.debug("Deque is empty and we're done retrieving URLs")
                break # if the deque is empty and we have all the urls our job is done
    log.debug("Exiting")
    return

def extract_storyurls(p):
    """Given a page from literotica, extract all of the storyurls present
    The passed object must be a string
    returns: a list of urls
    """
    s = BeautifulSoup(p)
    l = s.find_all(href=le_storyurl_regexp) # find all tags that have href as a keyword that match our literotica storyurl regexp
    l_t = []
    for i in l: # iterate through the list of matching urls
        l_t.append(i['href'])
    return l_t


if __name__ == '__main__':
    literoticarchiver()